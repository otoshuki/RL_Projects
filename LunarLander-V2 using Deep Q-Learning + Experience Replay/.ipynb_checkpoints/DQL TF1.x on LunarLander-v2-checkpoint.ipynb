{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate(Deep) Q-Learning\n",
    "## Using TF-1.x\n",
    "## Using Experience replay\n",
    "## Using Target networks\n",
    "## Comparision between with and without tweaks\n",
    "### Tested on LunarLander-v2\n",
    "\n",
    "##### Tried an oop approach, but I am not sure about TF1 graph flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the environment\n",
    "env = gym.make(\"CartPole-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Stuff to do beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF stuff\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow.compat.v1.keras as keras\n",
    "import tensorflow.compat.v1.keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking GPU Use\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AQL Agent\n",
    "\n",
    "### The loss function for our agent is defined as - \n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "Since $Q_{-}(s',a')$ is kept constant for our semi-grad Q-Learning, we will use `tf.stop_gradient` for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This has an experience replay buffer and uses target networks too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AQLAgent:\n",
    "    def __init__(self, env, epsilon=0.5, gamma=0.99, buffer_size=0, target_network=False):\n",
    "        '''\n",
    "        Use load=True to load a previously saved model\n",
    "        '''\n",
    "        #Set up constants\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.buffer = None\n",
    "        \n",
    "        #Set up model\n",
    "        self.model = self.create_model()\n",
    "        #Print out some details\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        #Placeholders for <s,a,r,s'> and game_end\n",
    "        self.states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "        self.rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "        self.next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])\n",
    "        \n",
    "        #Then performing Q-Learning we have\n",
    "        \n",
    "        #Get 𝑄𝜃(s,a)\n",
    "        self.pred_q = self.model(self.states_ph)\n",
    "        self.pred_q_for_a = tf.reduce_sum(self.pred_q * tf.one_hot(self.actions_ph, self.n_actions), axis=1)\n",
    "        \n",
    "        #Get Q_(s',a')\n",
    "        self.pred_next_q = self.model(self.next_states_ph)\n",
    "        #Get V_(s',a') using Q\n",
    "        self.next_v = tf.math.reduce_max(self.pred_next_q, axis=1)\n",
    "        #Get target Q-value, Q_(s',a')\n",
    "        self.target_q_for_a = self.rewards_ph + self.gamma*self.next_v\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        self.target_q_for_a = tf.where(self.is_done_ph, self.rewards_ph, self.target_q_for_a)\n",
    "        \n",
    "        #Calculate loss\n",
    "        self.loss = (self.pred_q_for_a - tf.stop_gradient(self.target_q_for_a)) ** 2\n",
    "        self.loss = tf.reduce_mean(self.loss)\n",
    "        \n",
    "        #Training function\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss)   \n",
    "        \n",
    "        #If buffer size is not 0\n",
    "        if buffer_size:\n",
    "            self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def create_model(self):\n",
    "        '''\n",
    "        Create a simple NN model\n",
    "        '''\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(L.InputLayer(self.state_dim))\n",
    "        model.add(L.Dense(400, kernel_initializer='uniform', activation='relu'))\n",
    "        model.add(L.Dense(400, kernel_initializer='uniform', activation='relu'))\n",
    "        #Output layer\n",
    "        model.add(L.Dense(self.n_actions, kernel_initializer='uniform', activation='linear'))\n",
    "        return model\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Uses E-greedy policy to get the agent action\n",
    "        '''\n",
    "        #Approximate the q_values\n",
    "        q_values = self.model.predict(state[None])[0]\n",
    "        #Explore or exploit\n",
    "        ore_or_oit = np.random.choice([0,1], p =[self.epsilon, 1-self.epsilon])\n",
    "        #If wanna explore\n",
    "        if ore_or_oit == 0:\n",
    "            chosen_action = np.random.choice(self.n_actions, 1)[0] #Over uniform dist\n",
    "        #If wanna exploit\n",
    "        else:\n",
    "            chosen_action = np.argmax(q_values)\n",
    "            \n",
    "        return chosen_action\n",
    "    \n",
    "    def generate_session(self, t_max=1000, train=False, batch_size=32):\n",
    "        '''\n",
    "        Run environment and train\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        s = self.env.reset()\n",
    "\n",
    "        for t in range(t_max):\n",
    "            a = self.get_action(s)       \n",
    "            next_s, r, done, _ = self.env.step(a)\n",
    "\n",
    "            if train:\n",
    "                sess.run(self.train_step,{\n",
    "                    self.states_ph: [s], self.actions_ph: [a], self.rewards_ph: [r], \n",
    "                    self.next_states_ph: [next_s], self.is_done_ph: [done]})\n",
    "                \n",
    "                #If experience replay is turned on\n",
    "                if self.buffer is not None:\n",
    "                    #Store data\n",
    "                    data = (s,a,r,next_s, done)\n",
    "                    self.buffer.add(*data)\n",
    "                    #Sample and run\n",
    "                    s_,a_,r_next_s_,done_ = self.buffer.sample(batch_size)\n",
    "                    for i in range(batch_size):\n",
    "                        sess.run(self.train_step,{\n",
    "                        self.states_ph: [s_[i]], self.actions_ph: [a_[i]], self.rewards_ph: [r_[i]], \n",
    "                        self.next_states_ph: [next_s_[i]], self.is_done_ph: [done_[i]]})\n",
    "                \n",
    "                        \n",
    "                    \n",
    "                \n",
    "            total_reward += r\n",
    "            s = next_s\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "    def save(self):\n",
    "        '''\n",
    "        Save the weights\n",
    "        '''\n",
    "        self.model.save_weights('./weights/cart_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "#Deque has a better time complexity\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        \"\"\"\n",
    "        self._storage = deque()\n",
    "        self._maxsize = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "    \n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Add new elements into the FIFO buffer\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        #FIFO Check!\n",
    "        if len(self._storage) >= self._maxsize:\n",
    "            #If not good, pop first element\n",
    "            self._storage.popleft(0)\n",
    "\n",
    "        #Now append the data\n",
    "        self._storage.append(data)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences.\n",
    "        \"\"\"\n",
    "        #Sample random indexes from the buffer\n",
    "        idxes = np.random.randint(len(self._storage), size=batch_size)\n",
    "\n",
    "        #First convert the data to numpy array\n",
    "        np_storage = np.array(self._storage)\n",
    "        \n",
    "        #Now use these indexes to get the samples\n",
    "        samples = np_storage[idxes]\n",
    "\n",
    "        #Return corresponding values\n",
    "        return(\n",
    "            samples[:,0],\n",
    "            samples[:,1],\n",
    "            samples[:,2],\n",
    "            samples[:,3],\n",
    "            samples[:,4]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = AQLAgent(env, buffer_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to train and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "mean_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "    session_rewards = [agent.generate_session(train=True) for _ in range(100)]\n",
    "    mean_rewards.append(np.mean(session_rewards))\n",
    "    agent.epsilon *= 0.99\n",
    "    clear_output(True)\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.show()\n",
    "    #Save weights after every iteration\n",
    "    agent.save()\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print(\"You Win! Stop using Keyboard Interrupt\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    agent.env = env_monitor\n",
    "    sessions = [agent.generate_session(train=False) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
