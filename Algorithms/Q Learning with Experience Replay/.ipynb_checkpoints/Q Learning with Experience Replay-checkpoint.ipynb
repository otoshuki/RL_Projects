{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm \n",
    "\n",
    "## - using Temporal Difference\n",
    "## - with $\\epsilon$ greedy exploration\n",
    "## - With Experience Replay\n",
    "\n",
    "### Tested on Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "import random, math\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Algorithm details - \n",
    "##### NB: Experience Reply explanation after this\n",
    "\n",
    "\n",
    "#### 1. Initlialize Q(s,a) with all zeros\n",
    "#### 2. Using the current Q(s,a) get the best action to take by following - \n",
    "\n",
    "##### 2.a - Get value function at current state, V(s) by -\n",
    "$$V(s) =   \\max_a Q(s,a)$$\n",
    "\n",
    "##### 2.b - Get the new Q function using -\n",
    "$$ \\hat Q(s_t,a_t) = r + \\gamma*V(s_t+1)$$\n",
    "\n",
    "##### 2.c - Smooth update of Q function using moving average -\n",
    "$$Q(s_t,a_t)=\\alpha*(\\hat Q(s_t,a_t)) + (1-\\alpha)*Q(s_t,a_t)$$\n",
    "Where, $\\alpha$ is the learning rate\n",
    "\n",
    "##### 2.d - Get the best action using - \n",
    "$$\\pi^*(s) = argmax_a Q(s,a)$$\n",
    "\n",
    "#### 3 - $\\epsilon$ - greedy exploration -\n",
    "Take a random action with probability $\\epsilon$, otherwise use best action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \n",
    "    #Step 1\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    #Step 2.a\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        #Go over all possible actions for the given state\n",
    "        #Take max to get V* basically\n",
    "        q_actions = []\n",
    "        for action in possible_actions:\n",
    "            q_actions.append(self.get_qvalue(state, action))\n",
    "        value = np.max(q_actions)\n",
    "\n",
    "        return value\n",
    "\n",
    "    #Steps 2.b, 2.c\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "        \n",
    "        #Get new_q first using V(s')\n",
    "        new_q = reward + gamma*self.get_value(next_state)\n",
    "        #Get moving averaged q_func with new_q and the older q_value\n",
    "        q_func_avg = learning_rate*new_q + (1-learning_rate)*self.get_qvalue(state, action)\n",
    "\n",
    "        self.set_qvalue(state, action, q_func_avg)\n",
    "\n",
    "    #Step 3\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values). \n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        #Best action is the argmax over the new functions\n",
    "        q_actions = []\n",
    "        for action in possible_actions:\n",
    "            q_actions.append(self.get_qvalue(state, action))\n",
    "        best_action = possible_actions[np.argmax(q_actions)]\n",
    "            \n",
    "        return best_action\n",
    "\n",
    "    #Step 4\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Taking into account E-Greedy Exploration!\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        ore_or_oit = np.random.choice([0,1], p =[epsilon, 1-epsilon])\n",
    "        #If wanna explore\n",
    "        if ore_or_oit == 0:\n",
    "            chosen_action = np.random.choice(possible_actions) #Over uniform dist\n",
    "        #If wanna exploit\n",
    "        else:\n",
    "            chosen_action = self.get_best_action(state)\n",
    "            \n",
    "        return chosen_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "#### Off-Policy algorithms like Q-Learning and EV-SARSA can find it difficult to improve over a state that seldom appears\n",
    "#### To improve over this, one can store the <s,a,r,s'> tuples from the sessions, sample from these previous instances and update upon it even if they aren't sampled under the current agent's policy\n",
    "\n",
    "## Algorithm details - \n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png width=480>\n",
    "\n",
    "#### 1. Run the session and update\n",
    "#### 2. Store the current <s,a,r,s'> transition in a FIFO buffer\n",
    "#### 3. Sample K such transitions from the buffer and update upon them\n",
    "#### 4. If the buffer is full, delete earliest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "    \n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Add new elements into the FIFO buffer\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        #FIFO Check!\n",
    "        if len(self._storage) >= self._maxsize:\n",
    "            #If not good, pop first element\n",
    "            self._storage.pop(0)\n",
    "\n",
    "        #Now append the data\n",
    "        self._storage.append(data)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        #Sample random indexes from the buffer\n",
    "        idxes = np.random.randint(len(self._storage), size=batch_size)\n",
    "\n",
    "        #First convert the data to numpy array\n",
    "        np_storage = np.array(self._storage)\n",
    "        \n",
    "        #Now use these indexes to get the samples\n",
    "        samples = np_storage[idxes]\n",
    "\n",
    "        #Return corresponding values\n",
    "        return(\n",
    "            samples[:,0],\n",
    "            samples[:,1],\n",
    "            samples[:,2],\n",
    "            samples[:,3],\n",
    "            samples[:,4]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open environment\n",
    "try:\n",
    "    env = gym.make('Taxi-v3')\n",
    "except gym.error.DeprecatedEnv:\n",
    "    # Taxi-v2 was replaced with Taxi-v3 in gym 0.15.0\n",
    "    env = gym.make('Taxi-v2')\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the agent\n",
    "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "                       get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create replay buffer\n",
    "replay = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train_with_replay(env, agent, replay=None,\n",
    "                               t_max=10**4, replay_batch_size=32):\n",
    "    \"\"\"\n",
    "    This function runs a full game will tmax\n",
    "    Stores the data and updates upon samples\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #Step1. get agent to pick action given state s\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        #Update agent on current transition\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        if replay is not None:\n",
    "            #Step 2. store current <s,a,r,s'> transition in buffer\n",
    "            data = (s,a,r,next_s,done)\n",
    "            replay.add(*data)\n",
    "\n",
    "            #Step3. Sample transitions and update on it\n",
    "            s_, a_, r_, next_s_, done_ = replay.sample(replay_batch_size)\n",
    "            for i in range(replay_batch_size):\n",
    "                agent.update(s_[i],a_[i],r_[i],next_s_[i])\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExpReplay: eps = 2.9191091959171894e-05 mean reward = 7.3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "def moving_average(x, span=100):\n",
    "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for i in range(1000):\n",
    "    rewards.append(\n",
    "        play_and_train_with_replay(env, agent, replay))\n",
    "    agent.epsilon *= 0.99\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('ExpReplay: eps =', agent.epsilon,\n",
    "              'mean reward =', np.mean(rewards[-10:]))\n",
    "        plt.plot(moving_average(rewards))\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Total Reward:  5.0\n"
     ]
    }
   ],
   "source": [
    "#Display\n",
    "total_reward = 0.\n",
    "agent.epsilon=0\n",
    "s = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    #sample action from policy\n",
    "    a = agent.get_action(s)\n",
    "    new_s, r, done, info = env.step(a)\n",
    "    env.render()\n",
    "    total_reward += r\n",
    "\n",
    "    s = new_s\n",
    "    if done:\n",
    "        break\n",
    "print(\"Total Reward: \", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
