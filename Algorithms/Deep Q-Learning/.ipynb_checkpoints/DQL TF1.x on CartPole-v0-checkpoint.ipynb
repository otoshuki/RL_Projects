{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate(Deep) Q-Learning\n",
    "## Using TF-1.x\n",
    "### Tested on CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the environment\n",
    "env = gym.make(\"CartPole-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Stuff to do beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF stuff\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow.compat.v1.keras as keras\n",
    "import tensorflow.compat.v1.keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Checking GPU Use\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras cannot save v1.x optimizers since they aren't compatible with checkpoints.\n",
    "#Need to save and load weights separately for this\n",
    "\n",
    "#If this runs, skip the next cell\n",
    "network = keras.models.load_model('./models/cart_model.h5')\n",
    "network.load_weights('./weights/cart_subs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AQL Agent\n",
    "\n",
    "### The loss function for our agent is defined as - \n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "Since $Q_{-}(s',a')$ is kept constant for our semi-grad Q-Learning, we will use `tf.stop_gradient` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AQLAgent:\n",
    "    def __init__(self, env, epsilon=0.1, gamma=0.99, load=False):\n",
    "        '''\n",
    "        Use load=True to load a previously saved model\n",
    "        '''\n",
    "        #Set up constants\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        \n",
    "        #Set up model\n",
    "        if load:\n",
    "            self.model = keras.model.load_model('./models/cart_model.h5')\n",
    "            self.model.load_weights('./weights/cart_weights')\n",
    "        else:\n",
    "            self.model = self.create_model()\n",
    "            self.model.save('./models/cart_model.h5')\n",
    "            #Print out some details\n",
    "            print(self.model.summary())\n",
    "        \n",
    "        #Placeholders for <s,a,r,s'> and game_end\n",
    "        self.states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "        self.rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "        self.next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])\n",
    "        \n",
    "        #Then performing Q-Learning we have\n",
    "        \n",
    "        #Get ùëÑùúÉ(s,a)\n",
    "        self.pred_q = self.model(self.states_ph)\n",
    "        self.pred_q_for_a = tf.reduce_sum(self.pred_q * tf.one_hot(self.actions_ph, self.n_actions), axis=1)\n",
    "        \n",
    "        #Get Q_(s',a')\n",
    "        self.pred_next_q = self.model(self.next_states_ph)\n",
    "        #Get V_(s',a') using Q\n",
    "        self.next_v = tf.math.reduce_max(self.pred_next_q, axis=1)\n",
    "        #Get target Q-value, Q_(s',a')\n",
    "        self.target_q_for_a = self.rewards_ph + self.gamma*self.next_v\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        self.target_q_for_a = tf.where(self.is_done_ph, self.rewards_ph, self.target_q_for_a)\n",
    "        \n",
    "        #Calculate loss\n",
    "        self.loss = (self.pred_q_for_a - tf.stop_gradient(self.target_q_for_a)) ** 2\n",
    "        self.loss = tf.reduce_mean(self.loss)\n",
    "        \n",
    "        #Training function\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss)   \n",
    "        \n",
    "    def create_model(self):\n",
    "        '''\n",
    "        Create a simple NN model\n",
    "        '''\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(L.InputLayer(self.state_dim))\n",
    "        model.add(L.Dense(400, kernel_initializer='uniform', activation='relu'))\n",
    "        model.add(L.Dense(400, kernel_initializer='uniform', activation='relu'))\n",
    "        #Output layer\n",
    "        model.add(L.Dense(self.n_actions, kernel_initializer='uniform', activation='linear'))\n",
    "        return model\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Uses E-greedy policy to get the agent action\n",
    "        '''\n",
    "        #Approximate the q_values\n",
    "        q_values = self.model.predict(state[None])[0]\n",
    "        #Explore or exploit\n",
    "        ore_or_oit = np.random.choice([0,1], p =[self.epsilon, 1-self.epsilon])\n",
    "        #If wanna explore\n",
    "        if ore_or_oit == 0:\n",
    "            chosen_action = np.random.choice(self.n_actions, 1)[0] #Over uniform dist\n",
    "        #If wanna exploit\n",
    "        else:\n",
    "            chosen_action = np.argmax(q_values)\n",
    "            \n",
    "        return chosen_action\n",
    "    \n",
    "    def generate_session(self, t_max=1000, train=False):\n",
    "        '''\n",
    "        Run environment and train\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        s = self.env.reset()\n",
    "\n",
    "        for t in range(t_max):\n",
    "            a = self.get_action(s)       \n",
    "            next_s, r, done, _ = self.env.step(a)\n",
    "\n",
    "            if train:\n",
    "                sess.run(self.train_step,{\n",
    "                    self.states_ph: [s], self.actions_ph: [a], self.rewards_ph: [r], \n",
    "                    self.next_states_ph: [next_s], self.is_done_ph: [done]})\n",
    "\n",
    "            total_reward += r\n",
    "            s = next_s\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 400)               2000      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 163,202\n",
      "Trainable params: 163,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agent = AQLAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AQLAgent' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-63533f29a156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msession_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmean_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-63533f29a156>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msession_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmean_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-af659ccb9026>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(self, t_max, train)\u001b[0m\n\u001b[1;32m     83\u001b[0m         '''\n\u001b[1;32m     84\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AQLAgent' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "mean_rewards = []\n",
    "\n",
    "for i in range(1000):\n",
    "    session_rewards = [agent.generate_session(train=True) for _ in range(100)]\n",
    "    mean_rewards.append(np.mean(session_rewards))\n",
    "    agent.epsilon *= 0.99\n",
    "    clear_output(True)\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.show()\n",
    "    #Save weights after every 10 iterations\n",
    "    if i%10 == 0:\n",
    "        agent.model.save_weights('./weights/cart_subs')\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
