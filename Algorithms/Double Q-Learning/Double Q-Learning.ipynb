{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learning\n",
    "## Using TF-2 Keras\n",
    "## Using Experience Replay\n",
    "\n",
    "### Tested on CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the environment\n",
    "env = gym.make(\"CartPole-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking GPU Use\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDNN(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OurDNN, self).__init__()\n",
    "        #Input layer\n",
    "        self.inp = L.InputLayer(input_dim)\n",
    "        #Hidden layers here - ReLu\n",
    "        self.hd1 = L.Dense(200, kernel_initializer='uniform', activation='relu')\n",
    "        self.hd2 = L.Dense(200, kernel_initializer='uniform', activation='relu')\n",
    "        #Output layer here - linear\n",
    "        self.out = L.Dense(output_dim, kernel_initializer='uniform', activation='linear')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input_data):\n",
    "        #Essentially feedforward your network\n",
    "        inp_fwd = self.inp(input_data)\n",
    "        hd1_fwd = self.hd1(inp_fwd)\n",
    "        hd2_fwd = self.hd2(hd1_fwd)\n",
    "        out_fwd = self.out(hd2_fwd)\n",
    "        #Get the output\n",
    "        return out_fwd        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our AQL Agent\n",
    "\n",
    "### The loss function for our agent is defined as - \n",
    "$$ L(w) = { 1 \\over N} \\sum_i (Q(s,a,w) - [r(s,a) + \\gamma \\cdot Q^{old}(s', argmax_{a^*}Q(s', a^*))]) ^2 $$\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "### The update equation is defined as\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha*\\nabla_{\\textbf w}L(\\textbf w)$$\n",
    "\n",
    "### Here we have two identical neural networks!\n",
    "#### 1.Current Q-network which will be used to calculate $Q(s,a)$\n",
    "#### 2. Old snapshot of the network will be used to calculate  $Q^{old}(s',a)$\n",
    "Where $a$ is $argmax_{a^*}Q(s', a^*)$\n",
    "\n",
    "This target network is updated at very target_steps time steps using the params of the Q-network - Hard Copy method\n",
    "\n",
    "Solves the problem of maximization bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurAgent:\n",
    "    def __init__(self, env, alpha=1e-4, epsilon=0.5, gamma=0.99, buffer_size=0, load=False):\n",
    "        \n",
    "        #Set up constants\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.optimizer = keras.optimizers.Adam(alpha)\n",
    "        self.buffer = None\n",
    "        \n",
    "        #Create the model - here, the Q-network\n",
    "        if load:\n",
    "            self.model = keras.models.load_model(\"./models/tf2_cart_model/\")\n",
    "            self.model.summary()\n",
    "            \n",
    "        else:\n",
    "            self.model = OurDNN(self.state_dim, self.n_actions)\n",
    "            self.model.compile(self.optimizer)\n",
    "            \n",
    "        #If buffer size is not 0\n",
    "        if buffer_size:\n",
    "            self.buffer = ReplayBuffer(buffer_size)\n",
    "            print(\"LOG: Using Experience Replay\")   \n",
    "        \n",
    "            #If loaded q-network, then directly load\n",
    "        if load:\n",
    "            #Doing this since I had trouble with copying weights\n",
    "            #from a pretrained model\n",
    "            self.old_model = keras.models.load_model(\"./models/tf2_cart_model/\")\n",
    "        #In case it is new\n",
    "        else:\n",
    "            self.old_model = OurDNN(self.state_dim, self.n_actions)\n",
    "            self.old_model.compile(self.optimizer)\n",
    "            self.q_snapshot()\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Uses E-greedy policy to get the agent action\n",
    "        '''\n",
    "        #Approximate the q_values\n",
    "        q_values = self.model.predict(state[None])[0]\n",
    "        #Explore or exploit\n",
    "        ore_or_oit = np.random.choice([0,1], p =[self.epsilon, 1-self.epsilon])\n",
    "        #If wanna explore\n",
    "        if ore_or_oit == 0:\n",
    "            chosen_action = np.random.choice(self.n_actions, 1)[0] #Over uniform dist\n",
    "        #If wanna exploit\n",
    "        else:\n",
    "            chosen_action = np.argmax(q_values)\n",
    "            \n",
    "        return chosen_action\n",
    "\n",
    "    def q_snapshot(self):\n",
    "        '''\n",
    "        Take a snapshot of Q network weights\n",
    "        '''\n",
    "        self.old_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def get_loss(self, state, action, next_state, reward, is_done):\n",
    "        '''\n",
    "        Get the loss function as defined above\n",
    "        '''\n",
    "        #Get ùëÑ(s,a) using our Q network model\n",
    "        pred_q = self.model(np.atleast_2d(state.astype('float32')))\n",
    "        pred_q_for_a = tf.reduce_sum(pred_q * tf.one_hot(action, self.n_actions), axis=1)\n",
    "        \n",
    "        #Get Q(s',a') using our Q network model\n",
    "        pred_next_q = self.model(np.atleast_2d(next_state.astype('float32')))\n",
    "        #Get the optimal action a* from Q(s',a')\n",
    "        opt_act = tf.math.argmax(pred_next_q, axis=1).numpy()[0]\n",
    "        #Get Q_old(s',a') using the snapshot model\n",
    "        pred_q_old = self.old_model(np.atleast_2d(next_state.astype('float32')))\n",
    "        #Get Q_old(s',a*)\n",
    "        pred_q_old = pred_q_old[:,opt_act]\n",
    "        \n",
    "        #Get target Q-value, Q_(s',a')\n",
    "        target_q_for_a = reward + self.gamma*pred_q_old\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_q_for_a = tf.where(is_done, reward, target_q_for_a)\n",
    "        \n",
    "        #Calculate loss\n",
    "        #Stop gradient is not required since we only update Q-net\n",
    "        loss = (pred_q_for_a - target_q_for_a) ** 2\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, state, action, next_state, reward, is_done):\n",
    "        '''\n",
    "        Trains the network\n",
    "        '''\n",
    "        #Variables to train - here weight\n",
    "        variables = self.model.trainable_variables\n",
    "        \n",
    "        #Perform semi-grad Q Learning with Adam optimizer\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(state, action, next_state, reward, is_done)\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "     \n",
    "    def generate_session(self, t_max=1000, train=False, batch_size=32, snap_steps=100):\n",
    "        '''\n",
    "        Run environment and train\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        s = self.env.reset()\n",
    "        \n",
    "        for t in range(t_max):\n",
    "            a = self.get_action(s)\n",
    "            next_s, r, done, _ = self.env.step(a)\n",
    "            \n",
    "            if train:\n",
    "                self.train_step(s, a, next_s, r, done)\n",
    "                \n",
    "                #If using exp replay to learn\n",
    "                if self.buffer is not None:\n",
    "                    data = (s, a, r, next_s, done)\n",
    "                    self.buffer.add(*data)\n",
    "                    #Sample transitions and update\n",
    "                    s_,a_,r_,next_s_,done_ = self.buffer.sample(batch_size)\n",
    "                    for i in range(batch_size):\n",
    "                        self.train_step(s_[i],\n",
    "                                        a_[i], \n",
    "                                        next_s_[i],\n",
    "                                        r_[i],\n",
    "                                        done_[i])\n",
    "                \n",
    "                #If using target_networks and after target_steps time\n",
    "                if (t%snap_steps==0):\n",
    "                    #Copy weights to target\n",
    "                    self.q_snapshot()\n",
    "            \n",
    "            total_reward += r\n",
    "            s = next_s\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save(\"./models/tf2_cart_model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experience replay buffer using deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "#Deque has a better time complexity\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        \"\"\"\n",
    "        self._storage = deque(maxlen=size)\n",
    "        self._maxsize = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "    \n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Add new elements into the FIFO buffer\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        #FIFO check not really required since dequeu checks it\n",
    "        #Now append the data\n",
    "        self._storage.append(data)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences.\n",
    "        \"\"\"\n",
    "        #Sample random indexes from the buffer\n",
    "        idxes = np.random.randint(len(self._storage), size=batch_size)\n",
    "\n",
    "        #First convert the data to numpy array\n",
    "        np_storage = np.array(self._storage)\n",
    "        \n",
    "        #Now use these indexes to get the samples\n",
    "        samples = np_storage[idxes]\n",
    "        #Return corresponding values\n",
    "        return(\n",
    "#             np.stack(samples[:,0]),\n",
    "            samples[:,0],\n",
    "            samples[:,1],\n",
    "            samples[:,2],\n",
    "#             np.stack(samples[:,3]),\n",
    "            samples[:,3],\n",
    "            samples[:,4]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent using cartpole environment and Adam optimizer\n",
    "agent = OurAgent(env, buffer_size=1024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "mean_rewards = []\n",
    "max_reward = 0\n",
    "try:\n",
    "    for i in range(1000):\n",
    "        session_rewards = agent.generate_session(train=True)\n",
    "        mean_rewards.append(np.mean(session_rewards))\n",
    "        agent.epsilon *= 0.99\n",
    "        clear_output(True)\n",
    "        print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.show()\n",
    "        #Save weights for new best weights\n",
    "        if mean_rewards[i] > max_reward:\n",
    "            max_reward = mean_rewards[i]\n",
    "            agent.save()\n",
    "        if np.mean(session_rewards) > 300:\n",
    "            print(\"You Win! Stop using Keyboard Interrupt\")\n",
    "    #         break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    clear_output(True)\n",
    "    print(\"Stopped\")\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.show()\n",
    "    #Commenting this saves the last best model\n",
    "#     agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    agent.env = env_monitor\n",
    "    agent.epsilon = 0 #For tests\n",
    "    sessions = [agent.generate_session(train=False) for _ in range(10)]\n",
    "    print(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
