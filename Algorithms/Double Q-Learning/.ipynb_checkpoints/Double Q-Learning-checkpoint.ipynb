{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learning\n",
    "## Using TF-2 Keras\n",
    "## Using Experience Replay\n",
    "\n",
    "### Tested on CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the environment\n",
    "env = gym.make(\"CartPole-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Checking GPU Use\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDNN(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OurDNN, self).__init__()\n",
    "        #Input layer\n",
    "        self.inp = L.InputLayer(input_dim)\n",
    "        #Hidden layers here - ReLu\n",
    "        self.hd1 = L.Dense(64, kernel_initializer='uniform', activation='relu')\n",
    "        self.hd2 = L.Dense(32, kernel_initializer='uniform', activation='relu')\n",
    "        #Output layer here - linear\n",
    "        self.out = L.Dense(output_dim, kernel_initializer='uniform', activation='linear')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input_data):\n",
    "        #Essentially feedforward your network\n",
    "        inp_fwd = self.inp(input_data)\n",
    "        hd1_fwd = self.hd1(inp_fwd)\n",
    "        hd2_fwd = self.hd2(hd1_fwd)\n",
    "        out_fwd = self.out(hd2_fwd)\n",
    "        #Get the output\n",
    "        return out_fwd        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our AQL Agent\n",
    "\n",
    "### The loss function for our agent is defined as - \n",
    "$$ L(w) = { 1 \\over N} \\sum_i (Q(s,a,w) - [r(s,a) + \\gamma \\cdot Q^{old}(s', argmax_{a^*}Q(s', a^*))]) ^2 $$\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "### The update equation is defined as\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha*\\nabla_{\\textbf w}L(\\textbf w)$$\n",
    "\n",
    "### Here we have two identical neural networks!\n",
    "#### 1.Current Q-network which will be used to calculate $Q(s,a)$\n",
    "#### 2. Old snapshot of the network will be used to calculate  $Q^{old}(s',a)$\n",
    "Where $a$ is $argmax_{a^*}Q(s', a^*)$\n",
    "\n",
    "This target network is updated at very target_steps time steps using the params of the Q-network - Hard Copy method\n",
    "\n",
    "Solves the problem of maximization bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurAgent:\n",
    "    def __init__(self, env, alpha=1e-4, epsilon=0.5, gamma=0.99, buffer_size=0, load=False):\n",
    "        \n",
    "        #Set up constants\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.optimizer = keras.optimizers.Adam(alpha)\n",
    "        self.buffer = None\n",
    "        \n",
    "        #Create the model - here, the Q-network\n",
    "        if load:\n",
    "            self.model = keras.models.load_model(\"./models/tf2_cart_model/\")\n",
    "            self.model.summary()\n",
    "            \n",
    "        else:\n",
    "            self.model = OurDNN(self.state_dim, self.n_actions)\n",
    "            self.model.compile(self.optimizer)\n",
    "            \n",
    "        #If buffer size is not 0\n",
    "        if buffer_size:\n",
    "            self.buffer = ReplayBuffer(buffer_size)\n",
    "            print(\"LOG: Using Experience Replay\")   \n",
    "        \n",
    "            #If loaded q-network, then directly load\n",
    "        if load:\n",
    "            #Doing this since I had trouble with copying weights\n",
    "            #from a pretrained model\n",
    "            self.old_model = keras.models.load_model(\"./models/tf2_cart_model/\")\n",
    "        #In case it is new\n",
    "        else:\n",
    "            self.old_model = OurDNN(self.state_dim, self.n_actions)\n",
    "            self.old_model.compile(self.optimizer)\n",
    "            self.q_snapshot()\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Uses E-greedy policy to get the agent action\n",
    "        '''\n",
    "        #Approximate the q_values\n",
    "        q_values = self.model.predict(state[None])[0]\n",
    "        #Explore or exploit\n",
    "        ore_or_oit = np.random.choice([0,1], p =[self.epsilon, 1-self.epsilon])\n",
    "        #If wanna explore\n",
    "        if ore_or_oit == 0:\n",
    "            chosen_action = np.random.choice(self.n_actions, 1)[0] #Over uniform dist\n",
    "        #If wanna exploit\n",
    "        else:\n",
    "            chosen_action = np.argmax(q_values)\n",
    "            \n",
    "        return chosen_action\n",
    "\n",
    "    def q_snapshot(self):\n",
    "        '''\n",
    "        Take a snapshot of Q network weights\n",
    "        '''\n",
    "        self.old_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def get_loss(self, state, action, next_state, reward, is_done):\n",
    "        '''\n",
    "        Get the loss function as defined above\n",
    "        '''\n",
    "        #Get ùëÑ(s,a) using our Q network model\n",
    "        pred_q = self.model(np.atleast_2d(state.astype('float32')))\n",
    "        pred_q_for_a = tf.reduce_sum(pred_q * tf.one_hot(action, self.n_actions), axis=1)\n",
    "        \n",
    "        #Get Q(s',a') using our Q network model\n",
    "        pred_next_q = self.model(np.atleast_2d(next_state.astype('float32')))\n",
    "        #Get the optimal action a* from Q(s',a')\n",
    "        opt_act = tf.math.argmax(pred_next_q, axis=1).value()\n",
    "        print(opt_act)\n",
    "        #Get Q_old(s',a') using the snapshot model\n",
    "        pred_q_old = self.old_model(np.atleast_2d(next_state.astype('float32')))\n",
    "        print(pred_q_old)\n",
    "        #Get Q_old(s',a*)\n",
    "        pred_q_old = pred_q_old[opt_act]\n",
    "        \n",
    "        #Get target Q-value, Q_(s',a')\n",
    "        target_q_for_a = reward + self.gamma*pred_q_old\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_q_for_a = tf.where(is_done, reward, target_q_for_a)\n",
    "        \n",
    "        #Calculate loss\n",
    "        #Stop gradient is not required since we only update Q-net\n",
    "        loss = (pred_q_for_a - target_q_for_a) ** 2\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, state, action, next_state, reward, is_done):\n",
    "        '''\n",
    "        Trains the network\n",
    "        '''\n",
    "        #Variables to train - here weight\n",
    "        variables = self.model.trainable_variables\n",
    "        \n",
    "        #Perform semi-grad Q Learning with Adam optimizer\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(state, action, next_state, reward, is_done)\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "     \n",
    "    def generate_session(self, t_max=1000, train=False, batch_size=32, target_steps=100):\n",
    "        '''\n",
    "        Run environment and train\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        s = self.env.reset()\n",
    "        \n",
    "        for t in range(t_max):\n",
    "            a = self.get_action(s)\n",
    "            next_s, r, done, _ = self.env.step(a)\n",
    "            \n",
    "            if train:\n",
    "                self.train_step(s, a, next_s, r, done)\n",
    "                \n",
    "                #If using exp replay to learn\n",
    "                if self.buffer is not None:\n",
    "                    data = (s, a, r, next_s, done)\n",
    "                    self.buffer.add(*data)\n",
    "                    #Sample transitions and update\n",
    "                    s_,a_,r_,next_s_,done_ = self.buffer.sample(batch_size)\n",
    "                    for i in range(batch_size):\n",
    "                        self.train_step(s_[i],\n",
    "                                        a_[i], \n",
    "                                        next_s_[i],\n",
    "                                        r_[i],\n",
    "                                        done_[i])\n",
    "                \n",
    "                #If using target_networks and after target_steps time\n",
    "                if (t%target_steps==0) and (self.target_model is not None):\n",
    "                    #Copy weights to target\n",
    "                    self.q_to_target()\n",
    "            \n",
    "            total_reward += r\n",
    "            s = next_s\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save(\"./models/tf2_cart_model2\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experience replay buffer using deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "#Deque has a better time complexity\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        \"\"\"\n",
    "        self._storage = deque(maxlen=size)\n",
    "        self._maxsize = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "    \n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Add new elements into the FIFO buffer\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        #FIFO check not really required since dequeu checks it\n",
    "        #Now append the data\n",
    "        self._storage.append(data)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences.\n",
    "        \"\"\"\n",
    "        #Sample random indexes from the buffer\n",
    "        idxes = np.random.randint(len(self._storage), size=batch_size)\n",
    "\n",
    "        #First convert the data to numpy array\n",
    "        np_storage = np.array(self._storage)\n",
    "        \n",
    "        #Now use these indexes to get the samples\n",
    "        samples = np_storage[idxes]\n",
    "        #Return corresponding values\n",
    "        return(\n",
    "#             np.stack(samples[:,0]),\n",
    "            samples[:,0],\n",
    "            samples[:,1],\n",
    "            samples[:,2],\n",
    "#             np.stack(samples[:,3]),\n",
    "            samples[:,3],\n",
    "            samples[:,4]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Using Experience Replay\n"
     ]
    }
   ],
   "source": [
    "#Agent using cartpole environment and Adam optimizer\n",
    "agent = OurAgent(env, buffer_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5.953725e-05 2.691942e-04]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "tf.Tensor([[-7.342404e-05 -9.268320e-05]], shape=(1, 2), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-496e0bdd501c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msession_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmean_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-496e0bdd501c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msession_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmean_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2f9e08d95403>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(self, t_max, train, batch_size, target_steps)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m#If using exp replay to learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2f9e08d95403>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, state, action, next_state, reward, is_done)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#Perform semi-grad Q Learning with Adam optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2f9e08d95403>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(self, state, action, next_state, reward, is_done)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_q_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#Get Q_old(s',a*)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mpred_q_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_q_old\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#Get target Q-value, Q_(s',a')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "mean_rewards = []\n",
    "max_reward = 0\n",
    "try:\n",
    "    for i in range(1000):\n",
    "        session_rewards = [agent.generate_session(train=True) for _ in range(10)]\n",
    "        mean_rewards.append(np.mean(session_rewards))\n",
    "        agent.epsilon *= 0.99\n",
    "        clear_output(True)\n",
    "        print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.show()\n",
    "        #Save weights for new best weights\n",
    "        if mean_rewards[i] > max_reward:\n",
    "            max_reward = mean_rewards[i]\n",
    "            agent.save()\n",
    "        if np.mean(session_rewards) > 300:\n",
    "            print(\"You Win! Stop using Keyboard Interrupt\")\n",
    "    #         break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    clear_output(True)\n",
    "    print(\"Stopped\")\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.show()\n",
    "    agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
