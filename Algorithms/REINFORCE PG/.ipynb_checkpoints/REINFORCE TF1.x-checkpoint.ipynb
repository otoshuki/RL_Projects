{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE algorithm\n",
    "# Policy Gradient based\n",
    "\n",
    "## Tested on CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient algorithms\n",
    "Given policy $\\pi(a/s)$, we have expected discounted reward,\n",
    "\n",
    "$$J = \\mathbb{E}_{s-p(s), a-\\pi_\\theta(s/a)}G(s,a) $$\n",
    "\n",
    "If we take J as the objective function and optimize our policy using gradient descent,\n",
    "\n",
    "$$\\theta_{i+1} \\leftarrow \\theta_{i} + \\alpha*\\nabla J$$\n",
    "Where, $\\theta$ is the policy parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/otoshuki/anaconda3/envs/tf2-gpu/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input variables, <S,A,R>\n",
    "ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n",
    "ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1.keras as keras\n",
    "from tensorflow.compat.v1.keras.layers import Dense, InputLayer\n",
    "\n",
    "network = keras.Sequential()\n",
    "network.add(InputLayer(state_dim))\n",
    "network.add(Dense(200, activation='relu'))\n",
    "network.add(Dense(200,activation='relu'))\n",
    "network.add(Dense(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 200)               1000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 41,602\n",
      "Trainable params: 41,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE algorithm\n",
    "\n",
    "#### Follows Policy gradient but with the following changes\n",
    "\n",
    "\n",
    "##### Given the objective function\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our policy is the softmax activation over network output\n",
    "#Since our actions are mutually exclusive-use softmax\n",
    "logits = network(ph_states)\n",
    "policy = tf.nn.softmax(logits)\n",
    "#Log policy for cost gradient calculation\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    \"\"\"\n",
    "    return policy.eval({ph_states: [states.astype(np.float32)]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative rewards\n",
    "$$G_t= r_t + \\gamma * G_{t + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \"\"\"\n",
    "    rewards.reverse()\n",
    "    cum_rewards = []\n",
    "    cum_rewards.append(rewards[0])\n",
    "    #Go through each reward\n",
    "    for i in range(len(rewards)-1):\n",
    "        cum_rewards.append(rewards[i+1] + gamma*cum_rewards[i])\n",
    "    cum_rewards.reverse()\n",
    "    return np.array(cum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Objective Function using MC\n",
    "##### Approx objective function\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select log pi(a_i/s_i) for the actions that were actually taken\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function with regularization\n",
    "#Cost function\n",
    "J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy regularization\n",
    "Without this, our policy will quickly tend to become deterministic\n",
    "\n",
    "$$ H(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = -tf.reduce_sum(policy*log_policy, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.1 #entropy regularization parameter\n",
    "loss = -(J + lamb*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)       \n",
    "        action_probs = predict_probs(s)\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)       \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, t_max=1000):\n",
    "    \"\"\"Given full session, trains agent with policy gradient\"\"\"\n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    update.run({\n",
    "        ph_states: states,\n",
    "        ph_actions: actions,\n",
    "        ph_cumulative_rewards: cumulative_rewards,\n",
    "    })\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 494.380\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "rewards_log = []\n",
    "\n",
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "    rewards_log.append(np.mean(rewards))\n",
    "    clear_output(True)\n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "    plt.plot(rewards_log)\n",
    "    plt.show()\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
