{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE algorithm\n",
    "\n",
    "## Using TF2\n",
    "## Tested on CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of actions: 2\n",
      "State dimension: (4,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "    \n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(\"No. of actions:\", n_actions)\n",
    "print(\"State dimension:\", state_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient algorithms\n",
    "Given policy $\\pi(a/s)$, we have expected discounted reward,\n",
    "\n",
    "$$J = \\mathbb{E}_{s-p(s), a-\\pi_\\theta(s/a)}G(s,a) $$\n",
    "\n",
    "If we take J as the objective function and optimize our policy using gradient descent,\n",
    "\n",
    "$$\\theta_{i+1} \\leftarrow \\theta_{i} + \\alpha*\\nabla J$$\n",
    "Where, $\\theta$ is the policy parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, InputLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDNN(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OurDNN, self).__init__()\n",
    "        #Input layer\n",
    "        self.inp = InputLayer(input_dim)\n",
    "        #Hidden layers here - ReLu\n",
    "        self.hd1 = Dense(200,activation='relu')\n",
    "        self.hd2 = Dense(200,activation='relu')\n",
    "        #Output layer here - linear\n",
    "        self.out = Dense(output_dim, kernel_initializer='uniform', activation='linear')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input_data):\n",
    "        #Essentially feedforward your network\n",
    "        inp_fwd = self.inp(input_data)\n",
    "        hd1_fwd = self.hd1(inp_fwd)\n",
    "        hd2_fwd = self.hd2(hd1_fwd)\n",
    "        out_fwd = self.out(hd2_fwd)\n",
    "        #Get the output\n",
    "        return out_fwd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, lr=1e-3, gamma=0.99, load=False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = keras.optimizers.Adam(lr)\n",
    "        \n",
    "        #Create network\n",
    "        if load:\n",
    "            self.network = keras.models.load_model(\"./models/tf2_pg_cartpole\")\n",
    "            self.network.summary()\n",
    "            print(\"Network loaded\")\n",
    "        else:\n",
    "            self.network = OurDNN(self.state_dim, self.n_actions)\n",
    "            self.network.compile(self.optimizer)\n",
    "            print(\"Network created\")\n",
    "            \n",
    "    def get_policies(self, states):\n",
    "        '''\n",
    "        Get the network output, given the states\n",
    "        Since actions are mutually exclusive - using softmax\n",
    "        Here, softmax and log_softmax\n",
    "        '''\n",
    "        logits = self.network(states)\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        log_policy = tf.nn.log_softmax(logits)\n",
    "        return policy, log_policy\n",
    "    \n",
    "    def get_G(self, rewards):\n",
    "        '''\n",
    "        Gets the cumulative reward G(s,a)\n",
    "        '''\n",
    "        rewards.reverse()\n",
    "        cum_rewards = []\n",
    "        cum_rewards.append(rewards[0])\n",
    "        #Go through each reward\n",
    "        for i in range(len(rewards)-1):\n",
    "            cum_rewards.append(rewards[i+1] + self.gamma*cum_rewards[i])\n",
    "        cum_rewards.reverse()\n",
    "        return np.array(cum_rewards).astype('float32')\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Get the sampled action using the current network\n",
    "        '''\n",
    "        policy, _ = self.get_policies(state)\n",
    "        return np.random.choice(self.n_actions, p=policy[0].numpy())\n",
    "    \n",
    "    def get_loss(self, states, actions, G, lamb=0.1):\n",
    "        '''\n",
    "        Get the loss function to optimize for\n",
    "        '''\n",
    "        #Get policy and log_policy first\n",
    "        policy, log_policy = self.get_policies(states)\n",
    "        #Log-policy for actions\n",
    "        indices = tf.stack([tf.range(tf.shape(log_policy)[0]), actions], axis=-1)\n",
    "        log_policy_for_actions = tf.gather_nd(log_policy, indices)\n",
    "        #Objective function\n",
    "        J = tf.reduce_mean(log_policy_for_actions*G)\n",
    "        #Entropy regularization\n",
    "        entropy = -tf.reduce_sum(policy*log_policy, axis=1)\n",
    "        #Get loss\n",
    "        loss = -(J + lamb*entropy)\n",
    "        return loss\n",
    "    \n",
    "    def train_on_session(self, states, actions, rewards):\n",
    "        '''\n",
    "        Given the training samples, update the parameters\n",
    "        '''\n",
    "        G = self.get_G(rewards)\n",
    "        variables = self.network.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(states, actions, G)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, variables))\n",
    "        return sum(rewards)\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save(\"./models/tf2_pg_cartpole\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myagent = Agent(env, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, t_max=1000):\n",
    "    '''\n",
    "    Play a full session with current agent\n",
    "    '''\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = agent.env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        #Get the action\n",
    "        s = s.astype(np.float32)\n",
    "        a = agent.get_action(s[None])        \n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        #Record\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return np.array(states), np.array(actions).astype('int32'), rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "rewards_log = []\n",
    "max_reward = 0\n",
    "\n",
    "try:\n",
    "    for i in range(100):\n",
    "        rewards = [myagent.train_on_session(*generate_session(myagent)) for _ in range(100)]  # generate new sessions\n",
    "        rewards_log.append(np.mean(rewards))\n",
    "        clear_output(True)\n",
    "        print(\"mean reward: %.3f at iter:\" % (np.mean(rewards)), i+1)\n",
    "        plt.plot(rewards_log)\n",
    "        plt.show()\n",
    "        if np.mean(rewards) > 300:\n",
    "            print(\"Yamerou!\") \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped\")\n",
    "    print(\"mean reward: %.3f at iter:\" % (np.mean(rewards)), i+1)\n",
    "    plt.plot(rewards_log)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
