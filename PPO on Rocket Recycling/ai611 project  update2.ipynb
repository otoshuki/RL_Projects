{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56d6787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 19:04:39.123880: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-07 19:04:39.123909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-07 19:04:39.124799: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-07 19:04:39.129677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 19:04:39.747099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as L\n",
    "from IPython.display import clear_output\n",
    "#ENV Stuff\n",
    "from rocket import Rocket\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a76db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 19:04:40.447764: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-07 19:04:40.471824: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-07 19:04:40.471951: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950c76d",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "#### We are using Zhengxia Zou, Rocket-recycling with Reinforcement Learning, 2021\n",
    "\n",
    "#### Repo Link: https://github.com/jiupinjia/rocket-recycling\n",
    "\n",
    "We will be focusing on the landing problem with rockets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1be31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'hover'\n",
    "num_steps = 800 #Environment steps\n",
    "env = Rocket(task=task, max_steps=num_steps)\n",
    "obs_N = env.state_dims #Observations are position, speed, angle, angle velocity, nozzle angle, and the simulation time\n",
    "act_N = env.action_dims #Actions are 3 thrust values in 3 directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a23131",
   "metadata": {},
   "source": [
    "## Agent Setup\n",
    "\n",
    "#### We wrote our own PPO-CLIP agent in Tensorflow 2\n",
    "Actor            |  Critic\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"actor.png\" alt=\"actor\" width=\"200\"/>  |  <img src=\"critic.png\" alt=\"critic\" width=\"200\"/>\n",
    "\n",
    "### Overall algorithm\n",
    "* Initialize actor(policy), critic networks\n",
    "* Collect trajectory(t=0 to T) in TRAJ\n",
    "    * a = sample($\\pi_{\\theta_k}(a|s)$)\n",
    "    * s', r, d = env.step(a)\n",
    "    * logprob = $log(\\pi_{\\theta_k}(a|s)))$\n",
    "* Calculate Advantage A using Generalized Advantage Estimation\n",
    "* Calculate Reward-to-go RTG\n",
    "* Update Actor\n",
    "    * Get $log(\\pi_\\theta(a|s))$ using TRAJ\n",
    "    * Calculate Diff: $log(\\pi_\\theta(a|s))-log(\\pi_{\\theta_k}(a|s))$\n",
    "    * Calculate Ratio: exp(diff)\n",
    "    * Calculate Loss: $\\mathbb{E}_t$[min(Ratio\\*A, clip(Ratio, 1-$\\epsilon$, 1+$\\epsilon$)\\*A)]\n",
    "    * Perform gradient descent on -Loss\n",
    "* Update Critic\n",
    "    * Get $V(s)$ using critic\n",
    "    * Calculate Diff: $V(s)-RTG$\n",
    "    * Calculate Loss: MSE(Diff)\n",
    "    * Perform gradient descent on Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1ac2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createActor(obs_N, act_N):\n",
    "    \"\"\"\n",
    "    Create Actor Model: Pi(a|s)\n",
    "    inp->fc1->relu->fc2->relu->fc3->softmax out\n",
    "    \"\"\"\n",
    "    inp = keras.Input(shape=obs_N, name='obs_input')\n",
    "    x = L.Dense(64, kernel_initializer='he_uniform', activation='relu', name='fc1')(inp)\n",
    "    x = L.Dense(32, kernel_initializer='he_uniform', activation='relu', name='fc2')(x)\n",
    "    out = L.Dense(act_N, kernel_initializer='he_uniform', activation='softmax', name='fc3')(x)\n",
    "    model = keras.Model(inputs=inp, outputs=out, name='actor')\n",
    "    return model\n",
    "\n",
    "def createCritic(obs_N):\n",
    "    \"\"\"\n",
    "    Create Critic Model: V(s)\n",
    "    inp->fc1->relu->fc2->relu->fc3->linear out\n",
    "    \"\"\"\n",
    "    inp = keras.Input(shape=obs_N, name='obs_input')\n",
    "    x = L.Dense(64, kernel_initializer='he_uniform', activation='relu', name='fc1')(inp)\n",
    "    x = L.Dense(32, kernel_initializer='he_uniform', activation='relu', name='fc2')(x)\n",
    "    out = L.Dense(1, kernel_initializer='he_uniform', name='fc3')(x)\n",
    "    model = keras.Model(inputs=inp, outputs=out, name='critic')\n",
    "    return model\n",
    "\n",
    "class PPOClipAgent:\n",
    "    def __init__(self, obs_N, act_N, actor_lr=1e-3, critic_lr=1e-3, gamma=0.9, clip=0.2, lmbda=0.95, summary=None):\n",
    "        #Agent parameters\n",
    "        self.obs_N = obs_N\n",
    "        self.act_N = act_N\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.gamma = gamma\n",
    "        #Networks\n",
    "        self.actor = createActor(obs_N, act_N)\n",
    "        self.critic = createCritic(obs_N)\n",
    "        #Optimizers\n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=actor_lr, clipvalue=1)\n",
    "        self.critic_optimizer = keras.optimizers.Adam(learning_rate=critic_lr, clipvalue=1)\n",
    "        #PPO specifics\n",
    "        self.clip = clip\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"\n",
    "        Return a discrete action from actor's softmax distribution\n",
    "        \"\"\"\n",
    "        action_probs = self.actor(np.atleast_2d(obs))\n",
    "        action = np.random.choice(np.arange(self.act_N), p=action_probs.numpy().flatten())\n",
    "        return action, action_probs\n",
    "\n",
    "    def get_gae_and_rtg(self, s, r, next_s, done, lengths):\n",
    "        \"\"\"\n",
    "        Use generalized advantage estimation of the current batch of samples\n",
    "        Also calculate the rewards to go, which is nothing but sum of discounted rewards from current state\n",
    "        \"\"\"\n",
    "        #Current critic estimate of V(s)\n",
    "        v_vals_s = self.critic(np.atleast_2d(s)).numpy()\n",
    "        #Current critic estimate of V(s')\n",
    "        #Please note this is essentially shifted i.e. s'(t)=s(t+1), but using it here to make implementation easy\n",
    "        v_vals_next_s = self.critic(np.atleast_2d(next_s)).numpy()\n",
    "        adv_full = np.zeros(len(r))\n",
    "        rtg_full = np.zeros(len(r))\n",
    "        #Buffer\n",
    "        start_id = 0\n",
    "        for lens in lengths:\n",
    "            T = start_id+lens\n",
    "            advantage = np.zeros(lens+1)\n",
    "            rewards_to_go = np.zeros(lens+1)\n",
    "            #td_T calculation\n",
    "            td_err = r[T] + self.gamma*v_vals_next_s[T]*(1-done[T]) - v_vals_s[T]\n",
    "            #r_T calculation\n",
    "            rewards_to_go[-1] = r[T]*(1-done[T])\n",
    "            for i in reversed(range(lens)):\n",
    "                t = start_id+i\n",
    "                #A_t = td_t + gamma*lambda*A_t+1\n",
    "                advantage[i] = td_err + self.gamma*self.lmbda*(1-done[t])*advantage[i+1]\n",
    "                #td_t-1 calculation\n",
    "                td_err = r[t] + self.gamma*v_vals_next_s[t]*(1-done[t]) - v_vals_s[t]\n",
    "                #Rewards to go\n",
    "                rewards_to_go[i] = r[t] +  self.gamma*(rewards_to_go[i+1])*(1-done[t])\n",
    "                \n",
    "            #Final entries are 0 for adv and rtg, we don't need them\n",
    "            adv_full[start_id:T] = advantage[:-1]\n",
    "            rtg_full[start_id:T] = rewards_to_go[:-1]\n",
    "            start_id = lens+1\n",
    "        print(T)\n",
    "        #Normalize advantage\n",
    "#         advantage = (advantage-advantage.mean())/(advantage.std() + 1e-10)\n",
    "        return adv_full, rtg_full\n",
    "\n",
    "    def train_step(self, s, a, r, next_s, done, probs, lengths):\n",
    "        \"\"\"\n",
    "        Train using experiences\n",
    "        \"\"\"\n",
    "        #Get advantage and rtg values\n",
    "        adv, rtg = self.get_gae_and_rtg(s, r, next_s, done, lengths)\n",
    "        print(adv)\n",
    "        print(rtg)\n",
    "#         adv = rtg - tf.squeeze(self.critic(s)).numpy() #Using it as numpy will remove computation dependence\n",
    "#         adv = (adv - adv.mean())/(adv.std()+1e-10)\n",
    "        #Actor Update\n",
    "        #We must ensure the original logprobs are not part of gradient calculation\n",
    "        #Squeeze is needed to ensure same size as other logprobs\n",
    "        logprobs = tf.squeeze(tf.math.log(probs)).numpy()\n",
    "        actor_learnables = self.actor.trainable_variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(actor_learnables)\n",
    "            curr_logprobs = tf.math.log(self.actor(s))\n",
    "            diff_logprobs = curr_logprobs-logprobs\n",
    "            ratio = tf.reduce_sum(tf.math.exp(diff_logprobs)*tf.one_hot(a, self.act_N), axis=1)\n",
    "            surrogate1 = ratio*adv\n",
    "            surrogate2 = tf.clip_by_value(ratio, 1-self.clip, 1+self.clip)*adv\n",
    "#             print(adv)\n",
    "#             if tf.math.reduce_mean(surrogate1-surrogate2) != 0:\n",
    "#                 print(\"ST\")\n",
    "#                 print(surrogate1)\n",
    "#                 print(surrogate2)\n",
    "#                 a = tf.math.minimum(surrogate1, surrogate2)\n",
    "#                 print(a-surrogate1)\n",
    "            actor_loss = tf.math.reduce_mean(-tf.math.minimum(surrogate1, surrogate2))\n",
    "            actor_grad = tape.gradient(actor_loss, actor_learnables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grad, actor_learnables))\n",
    "        #Critic Update\n",
    "        critic_learnables = self.critic.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            #Squeeze was needed to make sure the sizes align\n",
    "            v_vals_s = tf.squeeze(self.critic(s))\n",
    "            #Critic loss is mean(r-v)^2\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(rtg - v_vals_s))\n",
    "            critic_grad = tape.gradient(critic_loss, critic_learnables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, critic_learnables))\n",
    "        return actor_loss.numpy(), critic_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7425e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOClipAgent(obs_N, act_N, actor_lr=1e-3, critic_lr=1e-3, gamma=0.98, lmbda=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb9906",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "248e7b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1589"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9abec78c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n",
      "[2.84549761 2.84039497 2.83487415 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.83737016 2.83227062 2.82675052 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.82605267 2.82096577 2.81546021 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.81195951 2.8068943  2.80141401 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.79547977 2.79043794 2.78498673 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.77723956 2.7723031  2.76665688 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.75837088 2.75348258 2.7476809  ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.73857737 2.73372555 2.72795391 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.71849966 2.71420145 2.70871687 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.70300531 2.69868827 2.69322705 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.68933988 2.68516827 2.67984438 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.67606306 2.67142344 2.6659112  ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.66308212 2.65848708 2.6529963  ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.65035176 2.64575076 2.64024115 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.63779211 2.63313079 2.62754345 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.62529707 2.62052107 2.614995   ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.61270714 2.60805082 2.60270548 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.60123134 2.59727955 2.59231615 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.59427047 2.59020948 2.58521891 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n",
      "675\n",
      "[2.58768797 2.5830195  2.57778192 ... 0.         0.         0.        ]\n",
      "[6.62729537 6.59069513 6.55327057 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "num_iters = 1 #Training iterations\n",
    "num_epochs = 20 #Number of gradient steps\n",
    "num_eps = 5\n",
    "train_rewards = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "for it in range(num_iters):\n",
    "    #Empty buffers to store episode\n",
    "    s_buff = []\n",
    "    a_buff = []\n",
    "    r_buff = []\n",
    "    next_s_buff = []\n",
    "    done_buff = []\n",
    "    probs_buff = []\n",
    "    lengths_buff = []\n",
    "    #Reminder that done will not be stored since we train only at the end of episode\n",
    "    #Run environment num_eps times\n",
    "    for ep in range(num_eps):\n",
    "        #Reset env\n",
    "        s = env.reset()\n",
    "        for t in range(num_steps):\n",
    "            #Get action\n",
    "            a, a_prob = agent.get_action(s)\n",
    "            #Step env forward\n",
    "            next_s, r, done, info = env.step(a)\n",
    "            #Extra step for done at end of env steps\n",
    "            if t == num_steps-1:\n",
    "                done = True\n",
    "            #Batch collect samples\n",
    "            s_buff.append(s)\n",
    "            a_buff.append(a)\n",
    "            r_buff.append(r)\n",
    "            next_s_buff.append(next_s)\n",
    "            done_buff.append(done)\n",
    "            probs_buff.append(a_prob)\n",
    "            #Set next state\n",
    "            s = next_s\n",
    "            #If complete break out of current episode\n",
    "            if done:\n",
    "                #This will be useful in advantage and rtg estimation\n",
    "                lengths_buff.append(t)\n",
    "                break\n",
    "    #Ease of use with np arrays\n",
    "    s_buff = np.array(s_buff)\n",
    "    a_buff = np.array(a_buff)\n",
    "    r_buff = np.array(r_buff)\n",
    "    next_s_buff = np.array(next_s_buff)\n",
    "    done_buff = np.array(done_buff)\n",
    "    probs_buff = np.array(probs_buff)\n",
    "    #We are training only at the end of episode\n",
    "    a_loss_mean = 0\n",
    "    c_loss_mean = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        a_loss, c_loss = agent.train_step(s_buff, a_buff, r_buff, next_s_buff, done_buff, probs_buff, lengths_buff)\n",
    "        a_loss_mean += a_loss\n",
    "        c_loss_mean += c_loss\n",
    "    #Utilities\n",
    "#     cum_rew = np.sum(r_buff)/num_eps\n",
    "#     train_rewards.append(cum_rew)\n",
    "#     actor_losses.append(a_loss_mean/num_epochs)\n",
    "#     critic_losses.append(c_loss_mean/num_epochs)\n",
    "#     clear_output(True)\n",
    "#     plt.subplot(3, 1, 1)\n",
    "#     plt.title(\"rewards\")\n",
    "#     plt.plot(train_rewards)\n",
    "#     plt.plot(utils.moving_avg(train_rewards, N=10))\n",
    "#     plt.grid()\n",
    "#     plt.subplot(3, 1, 2)\n",
    "#     plt.title(\"ac loss\")\n",
    "#     plt.plot(actor_losses)\n",
    "#     plt.plot(utils.moving_avg(actor_losses, N=10))\n",
    "#     plt.grid()\n",
    "#     plt.subplot(3, 1, 3)\n",
    "#     plt.title(\"cr loss\")\n",
    "#     plt.plot(critic_losses)\n",
    "#     plt.plot(utils.moving_avg(critic_losses, N=10))\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "#     print('Iteration: {}, Reward: {}'.format(it, cum_rew))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4fdbe",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce666a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m cum_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m----> 4\u001b[0m     a, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(s)\n\u001b[1;32m      5\u001b[0m     next_s, r, done, _\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m      6\u001b[0m     frame_0, frame_1 \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[17], line 48\u001b[0m, in \u001b[0;36mPPOClipAgent.get_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mReturn a discrete action from actor's softmax distribution\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(np\u001b[38;5;241m.\u001b[39matleast_2d(obs))\n\u001b[0;32m---> 48\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_N), p\u001b[38;5;241m=\u001b[39maction_probs\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, action_probs\n",
      "File \u001b[0;32mmtrand.pyx:954\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cum_reward = 0\n",
    "for t in range(num_steps):\n",
    "    a, _ = agent.get_action(s)\n",
    "    next_s, r, done, _= env.step(a)\n",
    "    frame_0, frame_1 = env.render()\n",
    "    cv2.imshow('fr', frame_0[:,:,::-1])\n",
    "    cv2.waitKey(1)\n",
    "    cv2.imshow('fr', frame_1[:,:,::-1])\n",
    "    cv2.waitKey(1)\n",
    "    if done:\n",
    "        break\n",
    "    if env.already_crash:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965b7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
