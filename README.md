# RL_Projects

#### NB: Some of the algorithm notebooks are inspired/copied from - https://github.com/yandexdataschool/Practical_RL
#### Most of the algorithms are implemented as I go along with their Practical Reinforcement Learning Course on Coursera. Including the ones not included in the programming assignments
#### Linked here - https://www.coursera.org/learn/practical-rl/

## I've tested these algorithms on different Gym environments and gonna use it for my own project, so created this repo

Algorithms currently in the repo(or those I've studied):
1. Crossentropy
2. Deep Crossentropy
3. Value Iteration
4. Q-Learning with e - greedy exploration
5. SARSA
6. Expected Value SARSA with e-greedy exploration
7. Q-Learning with Experience Replay
8. Deep Q-Learning on TF1.x and TF2
9. Deep Q-Learning on TF2 with Experience replay and Target network
10. REINFORCE - Policy Gradient algorithm on TF1.x and TF2
11. Double Q-Learning on TF2 with Experience replay

Some basic implementations of Reinforcement Learning I've tried:
1. {14-12-18} CartPole-V0 using Deep Crossentropy
2. {15-12-18} LunarLander-V2 using Deep Crossentropy
3. {18-12-18} Frozen Lake using Value Iteration
4. {23-12-18} Taxi-V2 using Crossentropy
5. {26-12-18} Lunar Lander-V2 using Q Learning - FAILED
6. {13-08-20} CartPole-V0 using Q Learning with Experience Replay
7. {19-08-20} Lunar Lander-V2 using TF1.x Deep Q Learning and Experience Replay
8. {22-08-20} CartPole-V0 using TF2 DQN-Experience Replay and parallized envs - FAILED 
9. {23-08-20} LunarLander-V2 TF2 DQN-ExpRep-Target-Parallelized - FAILED
10. {26-08-20} LunarLander-v2 TF2 REINFORCE
11. {26-08-20} LunarLander-v2 TF2 DCEP with parallelized session generation
