{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic policy\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "2D arrat to represent policy(Uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.array([[(1/n_actions) for x in range(n_actions)] for y in range(n_states)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #sample action from policy\n",
    "        a = np.random.choice(n_actions, p=policy[s])\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        #Record state, action and add up reward to states,actions and total_reward accordingly. \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7600feb7f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(policy,t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards,bins=20);\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "    #reward_threshold = <Compute minimum reward for elite sessions. Hint: use np.percentile>\n",
    "    #Take the rewards above or equal to the 90th percentile\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    elite_states  = []\n",
    "    elite_actions = []\n",
    "\n",
    "    #Select states and actions from games with rewards >= percentile\n",
    "    for session in range(len(rewards_batch)):\n",
    "        #For sessions with rewards > percentile\n",
    "        if rewards_batch[session] >= reward_threshold:\n",
    "            #Consider the states and actions for the session as elite\n",
    "            for state in range(len(states_batch[session])):\n",
    "                elite_states.append(states_batch[session][state])\n",
    "                elite_actions.append(actions_batch[session][state])\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "    \n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "    \n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "    \n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    tot = 0\n",
    "    \n",
    "    for i in range(len(elite_states)):\n",
    "        new_policy[elite_states[i]][elite_actions[i]] += 1\n",
    "    for state in range(n_states):\n",
    "        if state not in elite_states:\n",
    "            new_policy[state] = [1/n_actions for action in range(n_actions)]\n",
    "        else:\n",
    "            tot = np.sum(new_policy[state])\n",
    "            new_policy[state] = new_policy[state]/tot\n",
    "    \n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(batch_rewards,range=reward_range);\n",
    "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
